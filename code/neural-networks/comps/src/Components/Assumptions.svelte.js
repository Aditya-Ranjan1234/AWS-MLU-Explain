import{SvelteComponent as e,init as n,safe_not_equal as t,element as s,space as o,text as a,attr as i,insert as r,append as l,noop as d,detach as p,HtmlTag as m}from"../../node_modules/svelte/internal/index.mjs.js";import h from"../katexify.js";function c(e){let n,t,c,u,f,b,g,w,v,y,x,T,H,q,L,M,_,I,z,W,j,k,A,B,F,D,E,R,C,N,S,V,G,J,K,O,P=h("y=B_0 + B_1x_1  + ... + B_px_p ",!1)+"";return{c(){n=s("h1"),n.textContent="Regression Model Assumptions",t=o(),c=s("p"),u=a("When teaching regression models, it's common to mention the various\n  assumptions underpinning linear regression. For completion, we'll list some of\n  those assumptions here. However, in the context of machine learning we care\n  most about if the predictions made from our model generalize well to unseen\n  data. We'll use our model if it generalizes well even if it violates\n  statistical assumptions. Still, no treatment of regression is complete without\n  mentioning the assumptions.\n  "),f=s("br"),b=o(),g=s("br"),w=o(),v=s("span"),v.innerHTML='<span class="dot svelte-2phlq9"></span> Validity:',y=a("\n  Does the data we're modeling matches to the problem we're actually trying to solve?\n\n  "),x=s("br"),T=o(),H=s("span"),H.innerHTML='<span class="dot svelte-2phlq9"></span> Representativeness:',q=a("\n  Is the sample data used to train the regression model representative of the population\n  to which it will be applied?\n\n  "),L=s("br"),M=o(),_=s("span"),_.innerHTML='<span class="dot svelte-2phlq9"></span> Additivity and Linearity:',I=a("\n  The deterministic component of a regression model is a linear function of the separate\n  predictors: "),z=new m(!1),W=a(".\n\n  "),j=s("br"),k=o(),A=s("span"),A.innerHTML='<span class="dot svelte-2phlq9"></span> Independence of Errors:',B=a("\n  The errors from our model are independent.\n  "),F=s("br"),D=o(),E=s("span"),E.innerHTML='<span class="dot svelte-2phlq9"></span> Homoscedasticity:',R=a("\n  The errors from our model have equal variance.\n  "),C=s("br"),N=o(),S=s("span"),S.innerHTML='<span class="dot svelte-2phlq9"></span> Normality of Errors:',V=a("\n  The errors from our model are normally distributed."),G=o(),J=s("br"),K=o(),O=s("p"),O.innerHTML='<span class="bold">When Assumptions Fail?</span> \n  <br/>\n  What should we do if the assumptions for our regression model aren&#39;t met? Don&#39;t\n  fret, it&#39;s not the end of the world! First, double-check that the assumptions even\n  matter in the first place: if the predictions made from our model generalize well\n  to unseen data, and our task is to create a model that generalizes well, then we&#39;re\n  probably fine. If not, figure out which assumption is being violated, and how to\n  address it! This will change depending on the assumption being violated, but in\n  general, one can attempt to extend the model, accompany new data, transform the\n  existing data, or some combination thereof. If a model transformation is unfit,\n  perhaps the application (or research question) can be changed or restricted to\n  better align with the data. In practice, some combination of the above will usually\n  suffice.',i(n,"class","body-header"),i(v,"class","bold"),i(H,"class","bold"),i(_,"class","bold"),z.a=W,i(A,"class","bold"),i(E,"class","bold"),i(S,"class","bold"),i(c,"class","body-text"),i(O,"class","body-text")},m(e,s){r(e,n,s),r(e,t,s),r(e,c,s),l(c,u),l(c,f),l(c,b),l(c,g),l(c,w),l(c,v),l(c,y),l(c,x),l(c,T),l(c,H),l(c,q),l(c,L),l(c,M),l(c,_),l(c,I),z.m(P,c),l(c,W),l(c,j),l(c,k),l(c,A),l(c,B),l(c,F),l(c,D),l(c,E),l(c,R),l(c,C),l(c,N),l(c,S),l(c,V),r(e,G,s),r(e,J,s),r(e,K,s),r(e,O,s)},p:d,i:d,o:d,d(e){e&&p(n),e&&p(t),e&&p(c),e&&p(G),e&&p(J),e&&p(K),e&&p(O)}}}class u extends e{constructor(e){super(),n(this,e,null,c,t,{})}}export{u as default};
