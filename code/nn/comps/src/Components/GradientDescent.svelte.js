import{SvelteComponent as e,init as t,safe_not_equal as n,binding_callbacks as i,bind as o,element as a,space as s,text as r,create_component as l,attr as c,insert as d,append as u,mount_component as h,set_input_value as f,listen as m,is_function as p,set_data as b,add_flush_callback as g,transition_in as w,transition_out as v,detach as y,destroy_component as x,run_all as _,component_subscribe as $,HtmlTag as k,to_number as S,action_destroyer as j,noop as G}from"../../node_modules/svelte/internal/index.mjs.js";import{gdBias as C,gdWeight as T}from"../store.js";import M from"../katexify.js";import z from"./GradientDescentScatterplot.svelte.js";import D from"./GradientDescentErrorPlot.svelte.js";import q from"./Hidden.svelte.js";import{tooltip as L}from"../tooltip.js";import{format as E}from"../../node_modules/d3-format/src/defaultLocale.js";function W(e){let t,n,i,o,l,h,f,m,p,b,g,w,v,x,_,$,S,C,T,z,D,q,E,W,A,H,I=M("\\begin{aligned} \\hat{y}=\\hat{\\beta_0} + \\hat{\\beta_1}x_1  \\end{aligned}",!0)+"",O=M("\\begin{aligned} MSE(\\hat{\\beta_0}, \\hat{\\beta_1}) = \\frac{1}{n} \\sum^{n}_{i=1}(y_i - \\hat{y_i})^2 \\\\\n        = \\frac{1}{n} \\sum^{n}_{i=1}(y_i - (\\hat{\\beta_0} + \\hat{\\beta_1}x_1 ))^2 \\end{aligned}",!0)+"",B=M("\\beta_0",!1)+"",N=M("\\beta_1",!1)+"",P=M("\\frac{\\delta}{\\delta\\beta_i}MSE = \\begin{cases}\n        -\\frac{2}{n} \\sum^{n}_{i=1}(y_i - \\hat{y_i}) \\text{for i = 0} \\\\\n        -\\frac{2}{n} \\sum^{n}_{i=1}x_i(y_i - \\hat{y_i}) \\text{for i = 1}\n        \\end{cases}",!0)+"",R=M("\\text{repeat until converge:} = \\begin{cases}\n         \\beta_0 = \\beta_0 - \\alpha (-\\frac{2}{n} \\sum^{n}_{i=1}(y_i - \\hat{y_i}))  \\\\\n         \\beta_1 = \\beta_1 - \\alpha (-\\frac{2}{n} x_i\\sum^{n}_{i=1}(y_i - \\hat{y_i})) \n        \\end{cases}",!0)+"";return{c(){t=a("section"),n=a("p"),i=r("Gradient descent works as follows. We assume that we have some convex\n      function representing the error of our machine learning algorithm (in our\n      case, MSE). Gradient descent will iteratively update our model's\n      coefficients in the direction of our error functions minimum "),o=a("span"),o.textContent="[â„¹]\n      ",l=r(".\n      "),h=a("br"),f=a("br"),m=r("\n      In our case, our model takes the form:\n      "),p=new k(!1),b=r("\n      and our error function takes the form:\n      "),g=new k(!1),w=r("\n\n      Our goal is to find the coefficients, "),v=new k(!1),x=r(" and\n      "),_=new k(!1),$=r(", to minimize the error function. To do\n      this, we'll use the gradient, which represents the direction that the function \n      is increasing, and the rate at which it is increasing. Since we want to find\n      the minimum of this function, we can go in the opposite direction of where it's \n      increasing. This is exactly what Gradient Descent does, it works by taking steps \n      in the direction opposite of where our error function is increasing, proportional \n      to the rate of change. To find the coefficients that minimize the function, we \n      first calculate the derivatives of our error function is increasing. To find \n      the coefficients that minimize first, calculate the derivatives of our loss \n      function, MSE:\n      "),S=new k(!1),C=r("\n\n      Now that we have the gradients for our error function (with respect\n      to each coefficient to be updated), we perform iterative updates:\n      "),T=new k(!1),z=s(),D=a("br"),q=a("br"),E=r("\n      Updating these values iteratively will yield coefficients of our model\n      that minimize error.\n      "),W=a("br"),c(o,"class","info-tooltip"),c(o,"title","Gradient descent won't always yield the best coefficients for our model, because it can sometimes \n    get stuck in local minima (as opposed to global minima). Many extensions exist to help solve this problem."),p.a=b,g.a=w,v.a=x,_.a=$,S.a=C,T.a=z,c(n,"class","body-text"),c(t,"class","gd-math svelte-48uix6")},m(e,a){d(e,t,a),u(t,n),u(n,i),u(n,o),u(n,l),u(n,h),u(n,f),u(n,m),p.m(I,n),u(n,b),g.m(O,n),u(n,w),v.m(B,n),u(n,x),_.m(N,n),u(n,$),S.m(P,n),u(n,C),T.m(R,n),u(n,z),u(n,D),u(n,q),u(n,E),u(n,W),A||(H=j(L.call(null,o)),A=!0)},p:G,d(e){e&&y(t),A=!1,H()}}}function A(e){let t,n,$,S,j,G,C,T,L,E,A,H,I,O,B,N,P,R,U,F,J,K,Q,V,X,Y,Z,ee,te,ne,ie,oe,ae,se,re,le,ce,de,ue,he,fe,me,pe,be,ge,we,ve,ye,xe,_e,$e,ke,Se,je,Ge,Ce,Te,Me,ze,De,qe,Le,Ee,We,Ae,He,Ie,Oe,Be,Ne,Pe,Re,Ue,Fe,Je,Ke,Qe,Ve,Xe,Ye,Ze=e[2]?"Hide":"Show",et=M("\\begin{aligned} \\hat{y}=\\hat{\\beta_0} + \\hat{\\beta_1}x_1  \\end{aligned}",!1)+"",tt=M("\\beta_0",!1)+"",nt=M("\\beta_1",!1)+"",it=M("\\hat{\\beta_0}",!1)+"",ot=e[6](e[4])+"",at=M("\\hat{\\beta_1}",!1)+"",st=e[6](e[5])+"",rt=M(`\\begin{aligned} y = ${e[6](e[5])}x${e[4]<0?"":"+"}${e[6](e[4])}+c \\end{aligned}`)+"";function lt(t){e[7](t)}function ct(t){e[8](t)}let dt={$$slots:{default:[W]},$$scope:{ctx:e}};void 0!==e[2]&&(dt.shown=e[2]),void 0!==e[3]&&(dt.show=e[3]),E=new q({props:dt}),i.push((()=>o(E,"shown",lt))),i.push((()=>o(E,"show",ct)));He=new z({props:{}}),e[15](He);return Be=new D({props:{}}),e[16](Be),{c(){t=a("h1"),t.textContent="Learning The Coefficients",n=s(),$=a("p"),$.innerHTML='<br/>\n  Let&#39;s recap what we&#39;ve learned so far: Linear regression is all about finding a\n  line (or surface) that fits our data well. And as we just saw, this involves selecting\n  the coefficients for our model that minimize our evaluation metric. But how can\n  we best estimate these coefficients? In practice, they&#39;re unknown, and selecting\n  them by hand quickly becomes infeasible for regression models with many features.\n  There must be a better way!\n  <br/><br/>\n  Luckily for us, several algorithms exist to do just this. We&#39;ll discuss two: an\n  iterative solution and a closed-form solution.\n  <br/><br/> \n  <span class="bold">An Iterative Solution</span> \n  <br/>\n  Gradient descent is an iterative optimization algorithm that estimates some set\n  of coefficients to yield the minimum of a convex function. Put simply: it will\n  find suitable coefficients for our regression model that minimize prediction error\n  (remember, lower MSE equals better model).\n  <br/><br/>\n  A full conversation on gradient descent is outside the course of this article (stay-tuned\n  for our future article on the subject), but if you&#39;d like to learn more, click\n  the &quot;Show Math&quot; button below. Otherwise, read on!\n  <br/>',S=s(),j=a("div"),G=a("button"),C=r(Ze),T=r(" Math"),L=s(),l(E.$$.fragment),I=s(),O=a("p"),B=a("br"),N=r("\n  Gradient descent will iteratively identify the coefficients our model needs to\n  fit the data. Let's see an example directly. We'll fit data to our equation \n  "),P=new k(!1),R=r(", so gradient descent will learn two coefficients, "),U=new k(!1),F=r(" (the intercept) and "),J=new k(!1),K=r(" (the weight). To do\n  so, interact with the plot below. Try dragging the weights and values to create a \n  'poorly' fit (large error) solution and run gradient descent to see the error \n  iteratively improve."),Q=s(),V=a("br"),X=a("br"),Y=s(),Z=a("div"),ee=a("div"),te=a("p"),te.textContent="Click the buttons to run 1, 10, or 100 steps of gradient descent, and see\n      the linear regression model update live. The error at each iteration of\n      gradient descent (or manual coefficient update) is shown in the bottom\n      chart. With each weight update, we recalculate the error, so you can see\n      how gradient descent improves our model iteratively.",ne=s(),ie=a("div"),oe=a("button"),oe.textContent="New Data",ae=s(),se=a("button"),se.textContent="1 Step",re=s(),le=a("button"),le.textContent="25 Steps",ce=s(),de=a("button"),de.textContent="100 Steps",ue=s(),he=a("div"),fe=a("div"),me=a("p"),pe=r("Bias ("),be=new k(!1),ge=r("): "),we=r(ot),ve=s(),ye=a("input"),xe=s(),_e=a("div"),$e=a("div"),ke=a("p"),Se=r("Weight ("),je=new k(!1),Ge=r("): "),Ce=r(st),Te=s(),Me=a("input"),ze=s(),De=a("div"),qe=r("Our model: "),Le=new k(!1),Ee=s(),We=a("div"),Ae=a("div"),l(He.$$.fragment),Ie=s(),Oe=a("div"),l(Be.$$.fragment),Ne=s(),Pe=a("br"),Re=a("br"),Ue=s(),Fe=a("p"),Fe.textContent="Although gradient descent is the most popular optimization algorithm in\n  machine learning, it's not perfect! It doesn't work for every loss function,\n  and it may not always find the most optimal set of coefficients for your\n  model. Still, it has many extensions to help solve these issues, and is widely\n  used across machine learning.",Je=s(),Ke=a("br"),Qe=a("br"),c(t,"class","body-header"),c($,"class","body-text"),c(G,"class","show-button svelte-48uix6"),c(j,"class","show-button-container svelte-48uix6"),P.a=R,U.a=F,J.a=K,c(O,"class","body-text"),c(te,"class","body-text"),c(oe,"class","svelte-48uix6"),c(se,"class","svelte-48uix6"),c(le,"class","svelte-48uix6"),c(de,"class","svelte-48uix6"),c(ie,"id","buttons-container"),c(ie,"class","svelte-48uix6"),be.a=ge,c(ye,"type","range"),c(ye,"min","-2"),c(ye,"step","0.5"),c(ye,"max","16"),c(ye,"class","slider"),c(ye,"id","myRange"),c(fe,"class","input-container svelte-48uix6"),c(he,"id","bias-slider"),je.a=Ge,c(Me,"type","range"),c(Me,"min","-1.5"),c(Me,"max","6"),c(Me,"step",".01"),c(Me,"class","slider"),c(Me,"id","myRange"),c($e,"class","input-container svelte-48uix6"),c(_e,"id","weight-slider"),Le.a=null,c(De,"id","equation-math"),c(ee,"id","equations-container"),c(ee,"class","svelte-48uix6"),c(Ae,"id","gd-chart-regression"),c(Ae,"class","svelte-48uix6"),c(Oe,"id","gd-chart-error"),c(We,"id","charts-container"),c(We,"class","svelte-48uix6"),c(Z,"id","gd-container"),c(Z,"class","svelte-48uix6"),c(Fe,"class","body-text")},m(i,o){d(i,t,o),d(i,n,o),d(i,$,o),d(i,S,o),d(i,j,o),u(j,G),u(G,C),u(G,T),d(i,L,o),h(E,i,o),d(i,I,o),d(i,O,o),u(O,B),u(O,N),P.m(et,O),u(O,R),U.m(tt,O),u(O,F),J.m(nt,O),u(O,K),d(i,Q,o),d(i,V,o),d(i,X,o),d(i,Y,o),d(i,Z,o),u(Z,ee),u(ee,te),u(ee,ne),u(ee,ie),u(ie,oe),u(ie,ae),u(ie,se),u(ie,re),u(ie,le),u(ie,ce),u(ie,de),u(ee,ue),u(ee,he),u(he,fe),u(fe,me),u(me,pe),be.m(it,me),u(me,ge),u(me,we),u(fe,ve),u(fe,ye),f(ye,e[4]),u(ee,xe),u(ee,_e),u(_e,$e),u($e,ke),u(ke,Se),je.m(at,ke),u(ke,Ge),u(ke,Ce),u($e,Te),u($e,Me),f(Me,e[5]),u(ee,ze),u(ee,De),u(De,qe),Le.m(rt,De),u(Z,Ee),u(Z,We),u(We,Ae),h(He,Ae,null),u(We,Ie),u(We,Oe),h(Be,Oe,null),d(i,Ne,o),d(i,Pe,o),d(i,Re,o),d(i,Ue,o),d(i,Fe,o),d(i,Je,o),d(i,Ke,o),d(i,Qe,o),Ve=!0,Xe||(Ye=[m(G,"click",(function(){p(e[3])&&e[3].apply(this,arguments)})),m(oe,"click",e[9]),m(se,"click",e[10]),m(le,"click",e[11]),m(de,"click",e[12]),m(ye,"change",e[13]),m(ye,"input",e[13]),m(Me,"change",e[14]),m(Me,"input",e[14])],Xe=!0)},p(t,[n]){e=t,(!Ve||4&n)&&Ze!==(Ze=e[2]?"Hide":"Show")&&b(C,Ze);const i={};131072&n&&(i.$$scope={dirty:n,ctx:e}),!A&&4&n&&(A=!0,i.shown=e[2],g((()=>A=!1))),!H&&8&n&&(H=!0,i.show=e[3],g((()=>H=!1))),E.$set(i),(!Ve||16&n)&&ot!==(ot=e[6](e[4])+"")&&b(we,ot),16&n&&f(ye,e[4]),(!Ve||32&n)&&st!==(st=e[6](e[5])+"")&&b(Ce,st),32&n&&f(Me,e[5]),(!Ve||48&n)&&rt!==(rt=M(`\\begin{aligned} y = ${e[6](e[5])}x${e[4]<0?"":"+"}${e[6](e[4])}+c \\end{aligned}`)+"")&&Le.p(rt);He.$set({});Be.$set({})},i(e){Ve||(w(E.$$.fragment,e),w(He.$$.fragment,e),w(Be.$$.fragment,e),Ve=!0)},o(e){v(E.$$.fragment,e),v(He.$$.fragment,e),v(Be.$$.fragment,e),Ve=!1},d(i){i&&y(t),i&&y(n),i&&y($),i&&y(S),i&&y(j),i&&y(L),x(E,i),i&&y(I),i&&y(O),i&&y(Q),i&&y(V),i&&y(X),i&&y(Y),i&&y(Z),e[15](null),x(He),e[16](null),x(Be),i&&y(Ne),i&&y(Pe),i&&y(Re),i&&y(Ue),i&&y(Fe),i&&y(Je),i&&y(Ke),i&&y(Qe),Xe=!1,_(Ye)}}}function H(e,t,n){let o,a,s,r,l,c;$(e,C,(e=>n(4,o=e))),$(e,T,(e=>n(5,a=e)));const d=E(".3f");return[s,r,l,c,o,a,d,function(e){l=e,n(2,l)},function(e){c=e,n(3,c)},()=>s.shuffleData(),()=>s.runGradientDescent(1),()=>s.runGradientDescent(25),()=>s.runGradientDescent(100),function(){o=S(this.value),C.set(o)},function(){a=S(this.value),T.set(a)},function(e){i[e?"unshift":"push"]((()=>{s=e,n(0,s)}))},function(e){i[e?"unshift":"push"]((()=>{r=e,n(1,r)}))}]}class I extends e{constructor(e){super(),t(this,e,H,A,n,{})}}export{I as default};
